# â˜ï¸ Crypto Data Platform (GCP + Python + Terraform)

![Build Status](https://github.com/carlolax/crypto-data-platform/actions/workflows/deploy.yaml/badge.svg)

A serverless, event-driven data engineering platform that ingests, processes, and analyzes cryptocurrency market data. This project uses **Infrastructure as Code (IaC)** to deploy a scalable, self-healing architecture on Google Cloud Platform and includes a **Hybrid Strategy Command Center** for visualization.

## ğŸ— Architecture

**Region:** `us-central1` (Iowa) - *Optimized for GCP Free Tier*

The pipeline follows a "Medallion Architecture" (Bronze â†’ Silver â†’ Gold), where each stage automatically triggers the next.

1.  **Ingestion (Bronze Layer):**
    * **Source:** CoinGecko API (`/coins/markets` endpoint).
    * **Features:**
        * **Rich Data:** Captures ATH, Circulating Supply, High/Low 24h, and Market Cap Rank.
        * **Smart Batching:** Automatically splits large coin lists into chunks to ensure scalability.
        * **Rate Limiting:** Built-in throttling to respect API limits (prevents 429 errors).
    * **Compute:** Google Cloud Function Gen 2 (Python 3.10).
    * **Trigger:** Cloud Scheduler (Hourly cron job) OR Client-Side Remote Control.
    * **Storage:** Google Cloud Storage (Raw JSON).
    * **Function:** `cdp-bronze-ingest-v2`

2.  **Processing (Silver Layer):**
    * **Trigger:** Event-Driven (Fires immediately when data lands in Bronze).
    * **Logic:** DuckDB (SQL-on-Serverless).
    * **Stability:** Uses a **"Download â†’ Process â†’ Upload"** pattern to handle memory constraints and prevent C++ threading crashes in the serverless environment.
    * **Features:**
        * **Historical Reconstruction:** Uses a "Wildcard Pattern" (`*.json`) to rebuild the entire dataset from history every run.
        * **Schema Evolution:** Automatically handles complex fields like `ath`, `circulating_supply`, and `max_supply`.
    * **Storage:** Google Cloud Storage (Parquet - Master History File).
    * **Function:** `cdp-silver-clean-v2`

3.  **Analytics (Gold Layer):**
    * **Trigger:** Event-Driven (Fires after Silver Layer completion).
    * **Logic:** DuckDB (SQL-on-Serverless).
    * **Features:**
        * **Financial Modeling:** Calculates 7-Day Simple Moving Averages (SMA) and Volatility (Standard Deviation).
        * **Algorithmic Signals:** Generates "BUY" (Dip), "SELL" (Rally), or "WAIT" signals based on Mean Reversion strategy.
    * **Storage:** Google Cloud Storage (Parquet - Analytics Ready).
    * **Function:** `cdp-gold-analytics-v2`

4.  **Visualization (The Command Center):**
    * **Tool:** Streamlit (Python-based UI).
    * **Mode:** Hybrid (Toggle between `LOCAL` disk data and `CLOUD` live bucket data).
    * **Features:** Interactive Plotly charts and financial metrics.

## ğŸ›  Tech Stack

* **Language:** Python 3.10
* **Infrastructure:** Terraform (IaC)
* **Data Processing:** Pandas (Ingest), DuckDB (OLAP Transformation)
* **Cloud:** Google Cloud Platform (Cloud Functions V2, Storage, Scheduler, IAM)
* **Visualization:** Streamlit, Plotly
* **Orchestration:** Eventarc (Triggers) & Custom Client Script (`run_pipeline.py`)

## ğŸ“‚ Project Structure

```text
.
â”œâ”€â”€ CONTRIBUTING.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ SECURITY.md
â”œâ”€â”€ data/                   # Local data storage (for testing)
â”‚   â”œâ”€â”€ bronze/             # Raw JSON files
â”‚   â”œâ”€â”€ gold/               # Final Aggregated Parquet files
â”‚   â””â”€â”€ silver/             # Cleaned Parquet files
â”œâ”€â”€ infra/                  # Terraform Infrastructure Code
â”‚   â”œâ”€â”€ bronze_layer_function.zip
â”‚   â”œâ”€â”€ budget.tf           # Billing alerts
â”‚   â”œâ”€â”€ functions.tf        # Cloud Function definitions (Source Zipping + Deployment)
â”‚   â”œâ”€â”€ gcp-key.json        # (Ignored) Service Account Key
â”‚   â”œâ”€â”€ gold_layer_function.zip
â”‚   â”œâ”€â”€ iam.tf              # Service Accounts & Permissions
â”‚   â”œâ”€â”€ provider.tf         # GCP Provider & Backend Config
â”‚   â”œâ”€â”€ scheduler.tf        # Cloud Scheduler (Cron Jobs)
â”‚   â”œâ”€â”€ silver_layer_function.zip
â”‚   â”œâ”€â”€ storage.tf          # GCS Bucket Definitions (Bronze/Silver/Gold)
â”‚   â”œâ”€â”€ terraform.tfstate   # (Ignored) State file
â”‚   â”œâ”€â”€ terraform.tfvars    # Configuration values (Region, IDs)
â”‚   â””â”€â”€ variables.tf        # Input variable declarations
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cloud_functions/    # Production-ready Cloud Functions
â”‚   â”‚   â”œâ”€â”€ bronze/         # Ingestion Logic (main.py + requirements.txt)
â”‚   â”‚   â”œâ”€â”€ gold/           # Analytics Logic (main.py + requirements.txt)
â”‚   â”‚   â””â”€â”€ silver/         # Transformation Logic (main.py + requirements.txt)
â”‚   â”œâ”€â”€ dashboard.py        # Hybrid Streamlit Dashboard
â”‚   â”œâ”€â”€ environment.yaml    # Conda Environment
â”‚   â”œâ”€â”€ pipeline/           # Local Data Pipeline Logic
â”‚   â”‚   â”œâ”€â”€ bronze/         # Local ingestion script (ingest.py)
â”‚   â”‚   â”œâ”€â”€ gold/           # Local analytics script (analyze.py)
â”‚   â”‚   â”œâ”€â”€ run_pipeline.py # Cloud Remote Control (Client-Side Trigger)
â”‚   â”‚   â””â”€â”€ silver/         # Local cleaning script (clean.py)
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ tests/                  # Unit Test Suite
    â”œâ”€â”€ test_bronze.py
    â””â”€â”€ test_silver.py
```

## âš™ï¸ CI/CD Automation
This project uses GitHub Actions to automate the infrastructure deployment, ensuring a "GitOps" workflow where code changes automatically reflect in the cloud.

- **Workflow**: `.github/workflows/deploy.yaml`
- **Trigger**: Pushes to the `main` branch (specifically for `infra/` or `src/cloud_functions/`).
- **Operations**:
    1. Setup: Authenticates via Workload Identity (Service Account Key).
    2. Lint: Runs `terraform fmt` to ensure code quality.
    3. Deploy: Runs `terraform apply` to update Google Cloud resources.
    4. State Management: Terraform State is stored remotely in a GCS Bucket to allow team collaboration and persistence.

## ğŸš€ Deployment Guide
**Prerequisites**
- Google Cloud SDK (gcloud) installed and authenticated.
- Terraform installed.
- Python 3.10+ installed.

### 1. Infrastructure Setup
**Option A: Automated (Recommended)** Simply commit your changes to the `main` branch. GitHub Actions will automatically provision and update the infrastructure.

**Option B: Manual (Dev/Debug)** Navigate to the infrastructure folder and apply the Terraform configuration manually.
```bash
cd infra
terraform init
terraform plan
terraform apply
```

### 2. Manual Trigger (Remote Control)
You can trigger the entire cloud pipeline directly from your local machine using the custom orchestrator script. This handles OIDC authentication and sends the trigger event.
```bash
# Ensure you are authenticated
gcloud auth application-default login

# Trigger the Cloud Pipeline
python src/pipeline/run_pipeline.py
```

### 3. Verification & Visualization
To see the results in the Strategy Command Center:
```bash
# Launch the Dashboard
streamlit run src/dashboard.py
```
*Note: Use the **"Data Source"** toggle in the sidebar to switch between `CLOUD` (Live) and `LOCAL` (Dev) modes instantly.*

## ğŸ§ª Local Development
To run the logic locally without deploying to the cloud:
```bash
# Activate environment
source crypto-env/bin/activate

# Run the Orchestrator
python src/pipeline/run_pipeline.py
```
*Alternatively, you can run individual layers manually:*
```bash
python src/pipeline/bronze/ingest.py
python src/pipeline/silver/clean.py
python src/pipeline/gold/analyze.py
```

## ğŸ›¡ Security
- **Service Account**: Uses a dedicated `crypto-runner-sa` with restricted permissions (`storage.admin`).
- **Idempotency**: All functions are designed to run multiple times without corrupting data (Overwrite logic).
- **Schema Enforcement**: Strict typing in DuckDB prevents pipeline crashes from bad API data.
- **Secret Management**: Sensitive keys are stored in GitHub Secrets and never committed to the repository.
